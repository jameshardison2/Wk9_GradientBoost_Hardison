{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b41051",
   "metadata": {},
   "source": [
    "# Week 9 — Gradient Boost (Capstone)\n",
    "**Author:** James Hardison II  \n",
    "**Date:** 2025-10-29\n",
    "\n",
    "This notebook applies **Gradient Boosting** to the Capstone dataset, aligned with BU DX799 Week 9 objectives. It includes:\n",
    "- Learning rate, number of estimators, and tree depth exploration\n",
    "- Regularization via subsampling and leaf constraints\n",
    "- Metrics, tuning, and feature importance\n",
    "- Short reflections per section for Milestone Two\n",
    "\n",
    "> ⚠️ Replace the **PLUG IN YOUR DATA** section with your real dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf9fbb",
   "metadata": {},
   "source": [
    "## 1. Project Context (Brief)\n",
    "- **Project title:** Predicting Kidney Disease Progression (example)\n",
    "- **Objective:** Predict `GFR` (or another clinical target) from demographics and labs.\n",
    "- **Dataset summary:** N rows, M features. Data sources: [describe].\n",
    "- **Target variable:** e.g., `gfr_cleaned` (regression)\n",
    "\n",
    "> Keep this section concise so peers from other domains can follow the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imports and Utility\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Helper to print metrics\n",
    "def regression_report(y_true, y_pred, label=\"model\"):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return pd.DataFrame({\n",
    "        \"model\":[label],\n",
    "        \"RMSE\":[rmse],\n",
    "        \"MAE\":[mae],\n",
    "        \"R2\":[r2]\n",
    "    })\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba36cf4",
   "metadata": {},
   "source": [
    "## 3. Data — PLUG IN YOUR DATA\n",
    "Replace the synthetic data block with data loading steps for your project.\n",
    "\n",
    "**Expected cell to customize:**\n",
    "- Load your data (CSV, SQL, etc.).\n",
    "- Define `numeric_features` and `categorical_features` if applicable.\n",
    "- Select `X` features and `y` target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c63792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example synthetic dataset for illustration (remove when using your real data)\n",
    "# Let's create a regression dataset with a few informative features.\n",
    "n = 1000\n",
    "X_df = pd.DataFrame({\n",
    "    \"age\": np.random.randint(20, 85, size=n),\n",
    "    \"creatinine\": np.random.gamma(shape=2.0, scale=0.8, size=n),\n",
    "    \"uacr\": np.random.gamma(shape=1.5, scale=30.0, size=n),\n",
    "    \"bmi\": np.random.normal(28, 5, size=n),\n",
    "    \"albumin\": np.random.normal(4.2, 0.5, size=n),\n",
    "})\n",
    "# True signal (toy): GFR decreases with creatinine, age; mild noise\n",
    "y = 120 - 12*X_df[\"creatinine\"] - 0.3*X_df[\"age\"] + 0.02*X_df[\"uacr\"] - 0.5*X_df[\"bmi\"] + np.random.normal(0, 5, size=n)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Basic numeric-only pipeline (imputer placeholder in case of missing values)\n",
    "numeric_features = list(X_df.columns)\n",
    "numeric_transform = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transform, numeric_features)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape, \" Test size:\", X_test.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3668f0c",
   "metadata": {},
   "source": [
    "## 4. Baseline Gradient Boost\n",
    "Train an initial model to set a baseline. Record RMSE/MAE/R2 and a brief interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb0 = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"gb\", GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "gb0.fit(X_train, y_train)\n",
    "pred0 = gb0.predict(X_test)\n",
    "baseline_metrics = regression_report(y_test, pred0, label=\"GB_baseline\")\n",
    "baseline_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc29a38",
   "metadata": {},
   "source": [
    "## 5. Learning Rate Study\n",
    "Compare a few learning rates with a fixed number of estimators and depth.\n",
    "Record RMSE vs. `learning_rate`. Smaller can generalize better but needs more trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e231678",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "results_lr = []\n",
    "for lr in learning_rates:\n",
    "    model = Pipeline(steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"gb\", GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=lr,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "    results_lr.append((lr, rmse))\n",
    "\n",
    "df_lr = pd.DataFrame(results_lr, columns=[\"learning_rate\", \"RMSE\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(df_lr[\"learning_rate\"], df_lr[\"RMSE\"], marker=\"o\")\n",
    "plt.title(\"RMSE vs Learning Rate\")\n",
    "plt.xlabel(\"learning_rate\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()\n",
    "\n",
    "df_lr.sort_values(\"RMSE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de601a5",
   "metadata": {},
   "source": [
    "## 6. Number of Estimators Curve\n",
    "Hold `learning_rate` constant. Increase `n_estimators` and observe train vs. test RMSE.\n",
    "Stop when test error plateaus to avoid overfitting and excess compute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n_list = [50, 100, 150, 200, 300, 400]\n",
    "train_rmse, test_rmse = [], []\n",
    "\n",
    "for n in n_list:\n",
    "    model = Pipeline(steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"gb\", GradientBoostingRegressor(\n",
    "            n_estimators=n,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_tr = model.predict(X_train)\n",
    "    pred_te = model.predict(X_test)\n",
    "    train_rmse.append(mean_squared_error(y_train, pred_tr, squared=False))\n",
    "    test_rmse.append(mean_squared_error(y_test, pred_te, squared=False))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(n_list, train_rmse, marker=\"o\", label=\"Train RMSE\")\n",
    "plt.plot(n_list, test_rmse, marker=\"o\", label=\"Test RMSE\")\n",
    "plt.title(\"RMSE vs n_estimators\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame({\"n_estimators\": n_list, \"Train_RMSE\": train_rmse, \"Test_RMSE\": test_rmse})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacb3e22",
   "metadata": {},
   "source": [
    "## 7. Regularization: Depth, Leaf Size, Subsample\n",
    "Control complexity to reduce overfitting:\n",
    "- `max_depth` → shallower trees generalize better\n",
    "- `min_samples_leaf` → prevents tiny leaves\n",
    "- `subsample` < 1.0 → randomness per tree\n",
    "\n",
    "Evaluate combinations and compare RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {\"max_depth\":2, \"min_samples_leaf\":5, \"subsample\":0.8},\n",
    "    {\"max_depth\":3, \"min_samples_leaf\":5, \"subsample\":0.8},\n",
    "    {\"max_depth\":3, \"min_samples_leaf\":10, \"subsample\":0.8},\n",
    "    {\"max_depth\":4, \"min_samples_leaf\":5, \"subsample\":0.7},\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for p in params:\n",
    "    model = Pipeline(steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"gb\", GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=p[\"max_depth\"],\n",
    "            min_samples_leaf=p[\"min_samples_leaf\"],\n",
    "            subsample=p[\"subsample\"],\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "    rows.append({**p, \"RMSE\": rmse})\n",
    "\n",
    "pd.DataFrame(rows).sort_values(\"RMSE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb49328",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning (GridSearchCV)\n",
    "Search a small grid to find a good balance. Report best params and CV score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603302cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"gb\", GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"gb__learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"gb__n_estimators\": [150, 200, 300],\n",
    "    \"gb__max_depth\": [2, 3, 4],\n",
    "    \"gb__min_samples_leaf\": [3, 5, 10],\n",
    "    \"gb__subsample\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "gcv = GridSearchCV(pipe, param_grid, scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1)\n",
    "gcv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", gcv.best_params_)\n",
    "print(\"CV RMSE:\", -gcv.best_score_)\n",
    "\n",
    "best_model = gcv.best_estimator_\n",
    "pred_best = best_model.predict(X_test)\n",
    "report_best = regression_report(y_test, pred_best, label=\"GB_best\")\n",
    "report_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e14fcb",
   "metadata": {},
   "source": [
    "## 9. Feature Importance\n",
    "Rank features by importance. Discuss clinical or domain sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract importance from the final GB step\n",
    "gb_step = best_model.named_steps[\"gb\"]\n",
    "importances = pd.Series(gb_step.feature_importances_, index=numeric_features).sort_values(ascending=False)\n",
    "importances.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a95280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "importances.sort_values(ascending=True).plot(kind=\"barh\")\n",
    "plt.title(\"Gradient Boost Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3c91f",
   "metadata": {},
   "source": [
    "## 10. Interpretation and Reflection\n",
    "- **Overfitting controls:** [Summarize which settings helped and evidence from RMSE curves.]\n",
    "- **Metrics used:** RMSE, MAE, R2. Best test RMSE = [value].\n",
    "- **Expected vs unexpected:** [Note any surprises in feature importance or curves.]\n",
    "- **EDA linkage:** [Explain how EDA guided feature selection and transformations.]\n",
    "- **Decision:** Is Gradient Boost a top candidate for Milestone Two depth? Why?\n",
    "\n",
    "> Keep this concise and Milestone-ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7ec0c",
   "metadata": {},
   "source": [
    "## 11. Sources (APA) + Yellowdig Note\n",
    "- Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. *Journal of Machine Learning Research*, 12, 2825–2830.\n",
    "- Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. *Annals of Statistics*, 1189–1232.\n",
    "- Scikit-learn User Guide (Gradient Boosting).\n",
    "\n",
    "**Yellowdig post idea:** Share the sklearn User Guide link and justify it as a high-quality source due to clear API docs, examples, and mathematical grounding.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
